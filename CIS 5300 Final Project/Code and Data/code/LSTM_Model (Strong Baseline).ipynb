{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"SVZsCREXDqM5"}},{"cell_type":"code","source":["# Imports\n","import json\n","import glob\n","import pandas as pd\n","import numpy as np\n","import re\n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib import cm\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","import string\n","import re\n","from tqdm import tqdm\n","from torch.utils.data import TensorDataset, DataLoader, Dataset"],"metadata":{"id":"kuu8ai6L2NAu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["is_cuda = torch.cuda.is_available()\n","\n","# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n","if is_cuda:\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is available\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU not available, CPU used\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yiHqqXsQ2hd1","executionInfo":{"status":"ok","timestamp":1703148219524,"user_tz":300,"elapsed":247,"user":{"displayName":"Henghak Kun (OwnedSoda13)","userId":"09749318087632614873"}},"outputId":"8535f9f0-cfc3-4bb4-f10f-bbb9499645bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is available\n"]}]},{"cell_type":"markdown","source":["# Dataset & GloVe"],"metadata":{"id":"fDzgmoTzDsyP"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ai4VGdO52j-0","executionInfo":{"status":"ok","timestamp":1703148220315,"user_tz":300,"elapsed":793,"user":{"displayName":"Henghak Kun (OwnedSoda13)","userId":"09749318087632614873"}},"outputId":"dfd30c67-465e-4afc-d3f4-b3c5cb59a21b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# This is the train df, which will be broken up into a training set and a validation set\n","# df = pd.read_csv(\"/content/drive/Shareddrives/CIS530 Final Project/datasets/train.csv\")\n","df = pd.read_csv(\"/content/drive/MyDrive/Copy of train.csv\")"],"metadata":{"id":"_C2ORxPb2mSV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test_df = pd.read_csv(\"/content/drive/Shareddrives/CIS530 Final Project/datasets/test.csv\")\n","test_df = pd.read_csv(\"/content/drive/MyDrive/Copy of test.csv\")"],"metadata":{"id":"v62Z6VCm2ndH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_en = df.loc[df['Language'] == 'en']\n","df_trun = df_en.sample(frac=1).groupby('Genre').head(1200)\n","X = df_trun['Lyrics']\n","Y = df_trun['Genre']\n","trainX, valX, trainY, valY = train_test_split(X, Y, test_size=0.15)"],"metadata":{"id":"hyvZaylB2t2J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainY"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g8xuNrWCCXLr","executionInfo":{"status":"ok","timestamp":1703148231188,"user_tz":300,"elapsed":103,"user":{"displayName":"Henghak Kun (OwnedSoda13)","userId":"09749318087632614873"}},"outputId":"0c7fe47e-0ea8-4c35-95e1-3b87a17985bf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["165696          Folk\n","173142    Electronic\n","151359         Metal\n","272143          Jazz\n","190867          Folk\n","             ...    \n","288566    Electronic\n","158758          Folk\n","280924    Electronic\n","204            Metal\n","279512    Electronic\n","Name: Genre, Length: 10200, dtype: object"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oR29rqP6Fj3w","executionInfo":{"status":"ok","timestamp":1703148231291,"user_tz":300,"elapsed":105,"user":{"displayName":"Henghak Kun (OwnedSoda13)","userId":"09749318087632614873"}},"outputId":"a25a27aa-98e0-40fd-a7a2-b8c2224bcd3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K8fkALA1XX-o","executionInfo":{"status":"ok","timestamp":1703148231291,"user_tz":300,"elapsed":4,"user":{"displayName":"Henghak Kun (OwnedSoda13)","userId":"09749318087632614873"}},"outputId":"b5def2c1-72a9-4452-ef6f-8c5a0d438bcd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["!wget -nc https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip\n","!unzip glove.6B.zip\n","!ls -lat"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0PnbVZDXkjrS","executionInfo":{"status":"ok","timestamp":1703148265004,"user_tz":300,"elapsed":33715,"user":{"displayName":"Henghak Kun (OwnedSoda13)","userId":"09749318087632614873"}},"outputId":"d61e27bc-98aa-4664-dca5-6110f1aec233"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["File ‘glove.6B.zip’ already there; not retrieving.\n","\n","Archive:  glove.6B.zip\n","replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n","  inflating: glove.6B.50d.txt        \n","total 3039152\n","drwxr-xr-x 1 root root       4096 Dec 21 08:44 .\n","drwx------ 5 root root       4096 Dec 21 08:07 drive\n","drwxr-xr-x 1 root root       4096 Dec 21 08:01 ..\n","drwxr-xr-x 1 root root       4096 Dec 19 14:20 sample_data\n","drwxr-xr-x 4 root root       4096 Dec 19 14:20 .config\n","-rw-r--r-- 1 root root  862182753 Aug 14  2021 glove.6B.zip\n","-rw-rw-r-- 1 root root  693434588 Dec 22  2015 glove.6B.200d.txt\n","-rw-rw-r-- 1 root root 1037965801 Dec 22  2015 glove.6B.300d.txt\n","-rw-rw-r-- 1 root root  171350515 Dec 22  2015 glove.6B.50d.txt\n","-rw-rw-r-- 1 root root  347117594 Dec 22  2015 glove.6B.100d.txt\n"]}]},{"cell_type":"code","source":["!head glove.6B.50d.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JRDqcqtAoQGE","executionInfo":{"status":"ok","timestamp":1703148265135,"user_tz":300,"elapsed":152,"user":{"displayName":"Henghak Kun (OwnedSoda13)","userId":"09749318087632614873"}},"outputId":"df538463-e5c6-4d76-cb3e-4c34fc3f7ef7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n",". 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n","of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n","to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n","and 0.26818 0.14346 -0.27877 0.016257 0.11384 0.69923 -0.51332 -0.47368 -0.33075 -0.13834 0.2702 0.30938 -0.45012 -0.4127 -0.09932 0.038085 0.029749 0.10076 -0.25058 -0.51818 0.34558 0.44922 0.48791 -0.080866 -0.10121 -1.3777 -0.10866 -0.23201 0.012839 -0.46508 3.8463 0.31362 0.13643 -0.52244 0.3302 0.33707 -0.35601 0.32431 0.12041 0.3512 -0.069043 0.36885 0.25168 -0.24517 0.25381 0.1367 -0.31178 -0.6321 -0.25028 -0.38097\n","in 0.33042 0.24995 -0.60874 0.10923 0.036372 0.151 -0.55083 -0.074239 -0.092307 -0.32821 0.09598 -0.82269 -0.36717 -0.67009 0.42909 0.016496 -0.23573 0.12864 -1.0953 0.43334 0.57067 -0.1036 0.20422 0.078308 -0.42795 -1.7984 -0.27865 0.11954 -0.12689 0.031744 3.8631 -0.17786 -0.082434 -0.62698 0.26497 -0.057185 -0.073521 0.46103 0.30862 0.12498 -0.48609 -0.0080272 0.031184 -0.36576 -0.42699 0.42164 -0.11666 -0.50703 -0.027273 -0.53285\n","a 0.21705 0.46515 -0.46757 0.10082 1.0135 0.74845 -0.53104 -0.26256 0.16812 0.13182 -0.24909 -0.44185 -0.21739 0.51004 0.13448 -0.43141 -0.03123 0.20674 -0.78138 -0.20148 -0.097401 0.16088 -0.61836 -0.18504 -0.12461 -2.2526 -0.22321 0.5043 0.32257 0.15313 3.9636 -0.71365 -0.67012 0.28388 0.21738 0.14433 0.25926 0.23434 0.4274 -0.44451 0.13813 0.36973 -0.64289 0.024142 -0.039315 -0.26037 0.12017 -0.043782 0.41013 0.1796\n","\" 0.25769 0.45629 -0.76974 -0.37679 0.59272 -0.063527 0.20545 -0.57385 -0.29009 -0.13662 0.32728 1.4719 -0.73681 -0.12036 0.71354 -0.46098 0.65248 0.48887 -0.51558 0.039951 -0.34307 -0.014087 0.86488 0.3546 0.7999 -1.4995 -1.8153 0.41128 0.23921 -0.43139 3.6623 -0.79834 -0.54538 0.16943 -0.82017 -0.3461 0.69495 -1.2256 -0.17992 -0.057474 0.030498 -0.39543 -0.38515 -1.0002 0.087599 -0.31009 -0.34677 -0.31438 0.75004 0.97065\n","'s 0.23727 0.40478 -0.20547 0.58805 0.65533 0.32867 -0.81964 -0.23236 0.27428 0.24265 0.054992 0.16296 -1.2555 -0.086437 0.44536 0.096561 -0.16519 0.058378 -0.38598 0.086977 0.0033869 0.55095 -0.77697 -0.62096 0.092948 -2.5685 -0.67739 0.10151 -0.48643 -0.057805 3.1859 -0.017554 -0.16138 0.055486 -0.25885 -0.33938 -0.19928 0.26049 0.10478 -0.55934 -0.12342 0.65961 -0.51802 -0.82995 -0.082739 0.28155 -0.423 -0.27378 -0.007901 -0.030231\n"]}]},{"cell_type":"code","source":["glove_path = '/content/glove.6B.50d.txt'\n","glove_embeddings = {}\n","with open(glove_path, 'r', encoding='utf-8') as file:\n","    for line in file:\n","        values = line.split()\n","        word = values[0]\n","        vector = np.array(values[1:], dtype='float32')\n","        glove_embeddings[word] = vector"],"metadata":{"id":"iywmHSmDZGcF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation Functions"],"metadata":{"id":"3jF-3zfFhC0C"}},{"cell_type":"code","source":["all_genres = set(df['Genre'])"],"metadata":{"id":"Oi2X_OK5ieli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Returns percent accuracy, (precision, recall, f1)\n","def evaluate(pred_labels, labels, genres):\n","    if len(pred_labels) != len(labels):\n","        return \"Label sizes are no the same\"\n","    tracker = {}\n","    total_corr = 0\n","    for x in genres:\n","        tracker[x] = {'count': 0, 'tp': 0, 'fp': 0, 'fn': 0}\n","    all_tp = 0\n","    all_fp = 0\n","    all_fn = 0\n","    for pred, actual in zip(pred_labels, labels):\n","        tracker[actual]['count'] = tracker[actual]['count'] + 1\n","        if pred == actual:\n","            tracker[pred]['tp'] = tracker[pred]['tp'] + 1\n","            all_tp += 1\n","            total_corr += 1\n","        else:\n","            tracker[actual]['fn'] = tracker[actual]['fn'] + 1\n","            tracker[pred]['fp'] = tracker[pred]['fp'] + 1\n","            all_fp += 1\n","            all_fn += 1\n","\n","    all_stats = {}\n","    for genre in tracker.keys():\n","        tp = tracker[genre]['tp']\n","        fp = tracker[genre]['fp']\n","        fn = tracker[genre]['fn']\n","        precision = tp/(tp+fp) if (tp+fp) > 0 else 0\n","        recall = tp/(tp+fn) if (tp+fn) > 0 else 0\n","        f1 = precision*recall/(precision+recall) if (precision+recall) > 0 else 0\n","        all_stats[genre] = {'count': tracker[genre]['count'], 'precision': precision, 'recall': recall, 'f1_score': f1}\n","    total_precision = all_tp/(all_tp+all_fp) if (all_tp+all_fp) > 0 else 0\n","    total_recall = all_tp/(all_tp+all_fn) if (all_tp+all_fn) > 0 else 0\n","    weighted_precision = sum([tracker[genre]['count']*all_stats[genre]['precision'] for genre in tracker.keys()])/len(pred_labels)\n","    weighted_recall = sum([tracker[genre]['count']*all_stats[genre]['recall'] for genre in tracker.keys()])/len(pred_labels)\n","    weighted_f1 = sum([tracker[genre]['count']*all_stats[genre]['f1_score'] for genre in tracker.keys()])/len(pred_labels)\n","    total_stats = {\n","        'macro_precision': total_precision,\n","        'macro_recall': total_recall,\n","        'macro_f1_score': total_precision*total_recall/(total_precision+total_recall)\n","            if (total_precision+total_recall) > 0 else 0,\n","        'weighted_precision': weighted_precision,\n","        'weighted_recall': weighted_recall,\n","        'weighted_f1_score': weighted_f1\n","    }\n","    return total_corr/len(pred_labels), total_stats, all_stats"],"metadata":{"id":"Re0eibGfhDzE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tabulate import tabulate\n","\n","def format_evaluation(stats):\n","    headers = list(all_genres)\n","    data = []\n","    for genre in headers:\n","        data.append([genre] + list(stats[2][genre].values()))\n","    res = tabulate(data, headers=['Genre', 'Count', 'Precision', 'Recall', 'F1_score'])\n","    res += '\\n\\nOverall\\n'\n","    res += 'Accuracy: ' + str(stats[0]) + '\\n' + \\\n","        'Weighted Precision: ' + str(stats[1]['weighted_precision']) + '\\n' + \\\n","        'Weighted Recall: ' + str(stats[1]['weighted_recall']) + '\\n' + \\\n","        'Weighted F1 Score: ' + str(stats[1]['weighted_f1_score']) + '\\n' + \\\n","        'Macro Precision: ' + str(stats[1]['macro_precision']) + '\\n' + \\\n","        'Macro Recall: ' + str(stats[1]['macro_recall']) + '\\n' + \\\n","        'Macro F1 Score: ' + str(stats[1]['macro_f1_score'])\n","    return res"],"metadata":{"id":"SLoKipGGhFU8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataloading & Tokenizing"],"metadata":{"id":"4Hj-m38Zaf_o"}},{"cell_type":"code","source":["stop_words = set(nltk.corpus.stopwords.words('english'))"],"metadata":{"id":"8qeHXNf0XYke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_text(text):\n","  if type(text) == float:\n","    print(text)\n","  tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n","  tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n","  tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords\n","  return tokens"],"metadata":{"id":"pUWy7Fd1Xb0-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_en = df.loc[df['Language'] == 'en']\n","df_trun = df_en.sample(frac=1).groupby('Genre').head(1200)"],"metadata":{"id":"z43DQmQOXet_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_trun['Processed_Lyrics'] = df_trun['Lyrics'].apply(preprocess_text)\n","\n","label_encoder = LabelEncoder()\n","df_trun['Genre_Encoded'] = label_encoder.fit_transform(df_trun['Genre'])"],"metadata":{"id":"cOfacmMvYuJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df['Processed_Lyrics'] = test_df['Lyrics'].apply(preprocess_text)\n","test_df['Genre_Encoded'] = label_encoder.fit_transform(test_df['Genre'])"],"metadata":{"id":"rmvx_moGe5vx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df, val_df = train_test_split(df_trun, test_size=0.15, random_state=42)"],"metadata":{"id":"si83aGY7Y257"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenization and creating word-to-index mapping\n","all_words = [word for tokens in train_df['Processed_Lyrics'] for word in tokens]\n","vocab = sorted(set(all_words))\n","word_to_idx = {word: idx for idx, word in enumerate(vocab)}"],"metadata":{"id":"0bQ-uKPsY4db"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an embedding matrix\n","embedding_dim = 50  # Assuming you're using 50-dimensional GloVe embeddings\n","embedding_matrix = np.zeros((len(vocab), embedding_dim))\n","for word, idx in word_to_idx.items():\n","    if word in glove_embeddings:\n","        embedding_matrix[idx] = glove_embeddings[word]"],"metadata":{"id":"TYrX65UcZNlU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a custom dataset class\n","class LyricsDataset(Dataset):\n","    def __init__(self, dataframe, word_to_idx, max_length):\n","        self.data = dataframe\n","        self.word_to_idx = word_to_idx\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        lyrics = self.data.iloc[idx]['Processed_Lyrics']\n","        encoded_lyrics = [self.word_to_idx[word] for word in lyrics if word in self.word_to_idx]\n","        padded_lyrics = encoded_lyrics[:self.max_length] + [0] * (self.max_length - len(encoded_lyrics))\n","        return torch.LongTensor(padded_lyrics), self.data.iloc[idx]['Genre_Encoded']"],"metadata":{"id":"kGufR28_ZPwg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"0vx24N34j49y"}},{"cell_type":"markdown","source":["## Model Architecture"],"metadata":{"id":"FqlHQC1qATVM"}},{"cell_type":"code","source":["# Define the LSTM model\n","class LSTMModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n","        super(LSTMModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        output, _ = self.lstm(embedded)\n","        output = self.fc(output[:, -1, :])\n","        return output"],"metadata":{"id":"G2-pmFhYZZff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set hyperparameters\n","vocab_size = len(vocab)\n","hidden_dim = 128\n","output_dim = len(df['Genre'].unique())  # Number of unique genres in your dataset\n","max_length = 100  # Set your desired maximum sequence length"],"metadata":{"id":"FFdECHLPZbQ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create datasets and dataloaders\n","train_dataset = LyricsDataset(train_df, word_to_idx, max_length)\n","val_dataset = LyricsDataset(val_df, word_to_idx, max_length)"],"metadata":{"id":"ZzbL3ivvZccZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = LyricsDataset(test_df, word_to_idx, max_length)"],"metadata":{"id":"dt33K38pfBO5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32  # Set your desired batch size\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)"],"metadata":{"id":"J5J0eaKsZd4E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loader = DataLoader(test_dataset, batch_size=batch_size)"],"metadata":{"id":"cMngbjAtfEYe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the model, loss function, and optimizer\n","model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"E4a1DtlsZfCg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training Loop"],"metadata":{"id":"QTWgkccoj8kE"}},{"cell_type":"code","source":["# Training loop\n","num_epochs = 10  # Set your desired number of epochs\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0.0\n","\n","    for inputs, labels in train_loader:  # Iterate through the DataLoader\n","        inputs = inputs.to(device)  # Move inputs to device\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhIa6h0nZg1M","executionInfo":{"status":"ok","timestamp":1703148344992,"user_tz":300,"elapsed":42423,"user":{"displayName":"Henghak Kun (OwnedSoda13)","userId":"09749318087632614873"}},"outputId":"0e90e9d6-d5e3-459b-b4e4-c633f5c57b64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 2.2283\n","Epoch [2/10], Loss: 2.1766\n","Epoch [3/10], Loss: 2.1100\n","Epoch [4/10], Loss: 2.0192\n","Epoch [5/10], Loss: 1.9115\n","Epoch [6/10], Loss: 1.7887\n","Epoch [7/10], Loss: 1.6442\n","Epoch [8/10], Loss: 1.4830\n","Epoch [9/10], Loss: 1.3322\n","Epoch [10/10], Loss: 1.1870\n"]}]},{"cell_type":"markdown","source":["## Validation Accuracy"],"metadata":{"id":"1M62LuV8je1W"}},{"cell_type":"code","source":["model.eval()\n","correct = 0\n","total = 0\n","\n","val_predictions = []\n","\n","with torch.no_grad():\n","    for inputs, labels in val_loader:\n","        inputs = inputs.to(device)\n","\n","        # Ensure labels are tensors and move them to the device\n","        if not isinstance(labels, torch.Tensor):\n","            labels = torch.tensor(labels).to(device)\n","\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs, 1)\n","\n","        val_predictions.append(predicted)\n","\n","        # Move predicted to the same device as labels for comparison\n","        predicted = predicted.to(device)\n","\n","        total += labels.size(0)\n","\n","        # Move labels to CPU for comparison\n","        correct += (predicted.cpu() == labels.cpu()).sum().item()\n","\n","val_accuracy = correct / total * 100\n","print(f\"Validation Accuracy: {val_accuracy:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cgzDxLBVeR2i","executionInfo":{"status":"ok","timestamp":1703148345778,"user_tz":300,"elapsed":799,"user":{"displayName":"Henghak Kun (OwnedSoda13)","userId":"09749318087632614873"}},"outputId":"f6b19203-3a37-4a0a-f007-81a4d10dcecd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 23.61%\n"]}]},{"cell_type":"code","source":["val_predictions_flat = [int(item) for tensor in val_predictions for item in tensor.tolist()]"],"metadata":{"id":"t9l81VBPfuNf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reverse_encoder = LabelEncoder()\n","reverse_encoder.fit(list(all_genres))\n","val_genre_preds = label_encoder.inverse_transform(val_predictions_flat)"],"metadata":{"id":"pUgRP-IAgKWY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_genre_preds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WlqfMh2UhXbk","executionInfo":{"status":"ok","timestamp":1703148345779,"user_tz":300,"elapsed":4,"user":{"displayName":"Henghak Kun (OwnedSoda13)","userId":"09749318087632614873"}},"outputId":"0547e1ec-3182-4dfc-d59b-9b0ed214e6c6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Hip-Hop', 'Rock', 'Pop', ..., 'Folk', 'Metal', 'Rock'],\n","      dtype=object)"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["print(format_evaluation(evaluate(val_genre_preds, val_df['Genre'], all_genres)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59jhfngmhZ_m","executionInfo":{"status":"ok","timestamp":1703148345899,"user_tz":300,"elapsed":123,"user":{"displayName":"Henghak Kun (OwnedSoda13)","userId":"09749318087632614873"}},"outputId":"2d8cfed8-68bb-4b7e-806c-1c8c4d1d22c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Genre         Count    Precision    Recall    F1_score\n","----------  -------  -----------  --------  ----------\n","Hip-Hop         164     0.552795  0.542683   0.273846\n","Rock            186     0.110345  0.172043   0.0672269\n","Indie           185     0.155039  0.108108   0.0636943\n","Pop             185     0.22      0.118919   0.077193\n","Metal           184     0.274611  0.288043   0.140584\n","R&B             167     0.221311  0.161677   0.0934256\n","Country         183     0.2       0.224044   0.10567\n","Jazz            190     0.348548  0.442105   0.194896\n","Electronic      189     0.183206  0.126984   0.075\n","Folk            167     0.144737  0.197605   0.0835443\n","\n","Overall\n","Accuracy: 0.2361111111111111\n","Weighted Precision: 0.23870737810538922\n","Weighted Recall: 0.2361111111111111\n","Weighted F1 Score: 0.11635736238849749\n","Macro Precision: 0.2361111111111111\n","Macro Recall: 0.2361111111111111\n","Macro F1 Score: 0.11805555555555555\n"]}]},{"cell_type":"markdown","source":["## Test Accuracy"],"metadata":{"id":"R8JqoIxpjbZB"}},{"cell_type":"code","source":["model.eval()\n","correct = 0\n","total = 0\n","\n","test_predictions = []\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs = inputs.to(device)\n","\n","        # Ensure labels are tensors and move them to the device\n","        if not isinstance(labels, torch.Tensor):\n","            labels = torch.tensor(labels).to(device)\n","\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs, 1)\n","\n","        test_predictions.append(predicted)\n","\n","        # Move predicted to the same device as labels for comparison\n","        predicted = predicted.to(device)\n","\n","        total += labels.size(0)\n","\n","        # Move labels to CPU for comparison\n","        correct += (predicted.cpu() == labels.cpu()).sum().item()\n","\n","val_accuracy = correct / total * 100\n","print(f\"Test Accuracy: {val_accuracy:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"12_jnNCPfI8L","executionInfo":{"status":"ok","timestamp":1703148349033,"user_tz":300,"elapsed":3136,"user":{"displayName":"Henghak Kun (OwnedSoda13)","userId":"09749318087632614873"}},"outputId":"84bf2322-bedc-4f6d-e1fb-6e818e7d9ad6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 23.62%\n"]}]},{"cell_type":"code","source":["test_predictions_flat = [int(item) for tensor in test_predictions for item in tensor.tolist()]\n","test_genre_preds = label_encoder.inverse_transform(test_predictions_flat)"],"metadata":{"id":"PN06F7cNhyBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(format_evaluation(evaluate(test_genre_preds, test_df['Genre'], all_genres)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tltjFiSUh9hG","executionInfo":{"status":"ok","timestamp":1703148349033,"user_tz":300,"elapsed":13,"user":{"displayName":"Henghak Kun (OwnedSoda13)","userId":"09749318087632614873"}},"outputId":"b00cd902-b0e7-4196-c287-2a1d4c6d73e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Genre         Count    Precision     Recall    F1_score\n","----------  -------  -----------  ---------  ----------\n","Hip-Hop         960    0.664642   0.569792    0.306786\n","Rock           1410    0.211915   0.176596    0.096325\n","Indie           510    0.0723684  0.0862745   0.039356\n","Pop            1110    0.211864   0.0900901   0.0632111\n","Metal           810    0.301994   0.392593    0.170692\n","R&B             510    0.124444   0.109804    0.0583333\n","Country         810    0.202105   0.237037    0.109091\n","Jazz            660    0.220794   0.286364    0.12467\n","Electronic      660    0.128165   0.122727    0.0626935\n","Folk            495    0.106987   0.19798     0.0694543\n","\n","Overall\n","Accuracy: 0.23616887208569629\n","Weighted Precision: 0.247510020884755\n","Weighted Recall: 0.23616887208569629\n","Weighted F1 Score: 0.11783029097960623\n","Macro Precision: 0.23616887208569629\n","Macro Recall: 0.23616887208569629\n","Macro F1 Score: 0.11808443604284814\n"]}]}]}